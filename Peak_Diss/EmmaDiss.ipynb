{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "62iiPEpGdPsC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from random import randint \n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, plot_confusion_matrix\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from orion.contrib.envs import load_env\n",
    "#load_env()\n",
    "import boto3\n",
    "#from orion.sources import S3Source\n",
    "aws_bucket = 'kilimanjaro-prod-datalake'\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zB7KgSFTfciI"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<swim_maha.SwimMaha at 0x7f620d47cb20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import swim_maha\n",
    "swim_maha.SwimMaha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WddsjVmgeVb_",
    "outputId": "4487f55b-59b4-4c85-d31b-e73ed7433624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mahakil_update.MAHAKIL at 0x7f617d170e50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mahakil_update as mahakil\n",
    "mahakil.MAHAKIL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z-qnMTUjaXJ-"
   },
   "outputs": [],
   "source": [
    "wine_white = pd.read_csv('winequality-white.csv', delimiter=';')\n",
    "wine_red = pd.read_csv('winequality-red.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSsIvFQ7aag6",
    "outputId": "4f69cdf3-05c5-4260-cb8f-a7229d4558f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1060\n",
      "1     183\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# White Wine Quality Low vs High\n",
    "WineWLvH = deepcopy(wine_white)\n",
    "WineWLvH = WineWLvH[(WineWLvH['quality'] <5)| (WineWLvH['quality'] >6)]\n",
    "WineWLvH.loc[WineWLvH.quality >= 7, \"class\"] = 0\n",
    "WineWLvH.loc[WineWLvH.quality <= 4, \"class\"] = 1\n",
    "WineWLvH['class'] = WineWLvH['class'].astype(\"int\")\n",
    "WineWLvH['class'] = WineWLvH['class'].astype(\"category\")\n",
    "WineWLvH = WineWLvH.drop(columns=['quality'])\n",
    "print(WineWLvH['class'].value_counts())\n",
    "WineWLvH.name = 'D10 - Wine_white_LvH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4AugjdXacvF",
    "outputId": "ac72b04f-cfa6-4e6e-8540-72429b369619"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1536\n",
       "1      63\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low Quality Red Wine (3,4)\n",
    "WineRL = deepcopy(wine_red)\n",
    "WineRL[\"class\"] = \"\"\n",
    "WineRL.loc[WineRL.quality >= 5, \"class\"] = 0\n",
    "WineRL.loc[WineRL.quality <= 4, \"class\"] = 1\n",
    "WineRL['class'] = WineRL['class'].astype(\"category\")\n",
    "WineRL = WineRL.drop(columns=['quality'])\n",
    "WineRL.name = 'D11 - Wine_red_low'\n",
    "WineRL['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BW8hcRTZafsO",
    "outputId": "da55806d-643d-4c0a-df4b-175aa77be18c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    217\n",
      "1     63\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Red Wine Quality Low vs High\n",
    "WineRLvH = deepcopy(wine_red)\n",
    "WineRLvH = WineRLvH[(WineRLvH['quality'] <5)| (WineRLvH['quality'] >6)]\n",
    "WineRLvH.loc[WineRLvH.quality >= 7, \"class\"] = 0\n",
    "WineRLvH.loc[WineRLvH.quality <= 4, \"class\"] = 1\n",
    "WineRLvH['class'] = WineRLvH['class'].astype(\"int\")\n",
    "WineRLvH['class'] = WineRLvH['class'].astype(\"category\")\n",
    "WineRLvH = WineRLvH.drop(columns=['quality'])\n",
    "print(WineRLvH['class'].value_counts())\n",
    "WineRLvH.name ='D12 - Wine_red_LvH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SgaEXmOJagWJ"
   },
   "outputs": [],
   "source": [
    "vowel = pd.read_csv('vowel_data.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "J6ytm0mtaiS8"
   },
   "outputs": [],
   "source": [
    "vowel = vowel.drop(columns=[10])\n",
    "vowel = vowel.dropna()\n",
    "cols_to_check = [0,9]\n",
    "vowel[cols_to_check] = vowel[cols_to_check].replace({'{':''}, regex=True)\n",
    "vowel[cols_to_check] = vowel[cols_to_check].replace({'}':''}, regex=True)\n",
    "vowel = vowel.reset_index(drop=True)\n",
    "vowel[9] = pd.to_numeric(vowel[9])\n",
    "vowel = vowel.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ilmKyRKrall-"
   },
   "outputs": [],
   "source": [
    "speakers = ['Andrew', 'Bill', 'David', 'Mark', 'Jo', 'Kate', 'Penny', 'Rose', 'Mike', 'Nick', 'Rich', 'Tim', 'Sarah', 'Sue', 'Wendy']\n",
    "words = ['hid', 'hId', 'hEd', 'hAd', 'hYd', 'had', 'hOd', 'hod', 'hUd', 'hud', 'hed']\n",
    "segments = [0,1,2,3,4,5]\n",
    "speakers_1 = []\n",
    "words_1 = []\n",
    "segment = []\n",
    "for s in speakers:\n",
    "  for se in segments:\n",
    "    for w in words:\n",
    "      speakers_1.append(s)\n",
    "      segment.append(se)\n",
    "      words_1.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sivKeb4LampC"
   },
   "outputs": [],
   "source": [
    "vowel['word'] = np.arange(len(vowel))\n",
    "vowel['speaker'] = np.arange(len(vowel))\n",
    "vowel['segment'] = np.arange(len(vowel))\n",
    "for l in range(len(vowel)):\n",
    "  vowel['speaker'] = vowel['speaker'].replace(l, speakers_1[l])\n",
    "  vowel['word'] = vowel['word'].replace(l, words_1[l])\n",
    "  vowel['segment'] = vowel['segment'].replace(l, segment[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "u7psCi70aqdu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    900\n",
      "1     90\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "vowel['class']= ''\n",
    "# Vowel 0 \n",
    "vowel_0 = deepcopy(vowel)\n",
    "vowel_0.loc[vowel_0.word != 'hid', \"class\"] = 0\n",
    "vowel_0.loc[vowel_0.word == 'hid', \"class\"] = 1\n",
    "vowel_0['class'] = vowel_0['class'].astype(\"int\")\n",
    "vowel_0['class'] = vowel_0['class'].astype(\"category\")\n",
    "vowel_0 = vowel_0.drop(columns=['word','speaker','segment'])\n",
    "print(vowel_0['class'].value_counts())\n",
    "vowel_0.name = 'D3 - Vowel_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHC0ZiXDasv2",
    "outputId": "544641f8-dc0d-4213-81f4-e6928cdcfd61",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    900\n",
      "1     90\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vowel 2\n",
    "vowel_2 = deepcopy(vowel)\n",
    "vowel_2.loc[vowel_2.word != 'hEd', \"class\"] = 0\n",
    "vowel_2.loc[vowel_2.word == 'hEd', \"class\"] = 1\n",
    "vowel_2['class'] = vowel_2['class'].astype(\"int\")\n",
    "vowel_2['class'] = vowel_2['class'].astype(\"category\")\n",
    "vowel_2 = vowel_2.drop(columns=['word','speaker','segment'])\n",
    "print(vowel_2['class'].value_counts())\n",
    "vowel_2.name = 'D4 - Vowel_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ = 'masters/datascience/emma/retail.csv'\n",
    "retail = s3.get_object(Bucket=aws_bucket, Key=object_)\n",
    "retail = pd.read_csv(retail['Body'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASP</th>\n",
       "      <th>cluster</th>\n",
       "      <th>loyaltyaccount_No</th>\n",
       "      <th>loyaltyaccount_Yes</th>\n",
       "      <th>gender_female</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>gender_unknown</th>\n",
       "      <th>shipcountry_Albania</th>\n",
       "      <th>shipcountry_Armenia</th>\n",
       "      <th>shipcountry_Australia</th>\n",
       "      <th>...</th>\n",
       "      <th>category_Childrens</th>\n",
       "      <th>category_Infant</th>\n",
       "      <th>category_Junior</th>\n",
       "      <th>category_Mens</th>\n",
       "      <th>category_Miscellaneous</th>\n",
       "      <th>category_Nursery</th>\n",
       "      <th>category_Womens</th>\n",
       "      <th>divisioncode_ACCESSORY</th>\n",
       "      <th>divisioncode_APPAREL</th>\n",
       "      <th>divisioncode_FOOTWEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ASP  cluster  loyaltyaccount_No  loyaltyaccount_Yes  gender_female  \\\n",
       "0  59.92        0                  0                   1              1   \n",
       "\n",
       "   gender_male  gender_unknown  shipcountry_Albania  shipcountry_Armenia  \\\n",
       "0            0               0                    0                    0   \n",
       "\n",
       "   shipcountry_Australia  ...  category_Childrens  category_Infant  \\\n",
       "0                      0  ...                   0                0   \n",
       "\n",
       "   category_Junior  category_Mens  category_Miscellaneous  category_Nursery  \\\n",
       "0                0              1                       0                 0   \n",
       "\n",
       "   category_Womens  divisioncode_ACCESSORY  divisioncode_APPAREL  \\\n",
       "0                0                       0                     1   \n",
       "\n",
       "   divisioncode_FOOTWEAR  \n",
       "0                      0  \n",
       "\n",
       "[1 rows x 85 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail = retail.rename(columns={'class':'cluster'})\n",
    "retail.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1935848\n",
      "2     245926\n",
      "1      23099\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(retail['cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.877986\n",
       "2    0.111537\n",
       "1    0.010476\n",
       "Name: cluster, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail['cluster'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1958947\n",
      "1     245926\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cust Summary focusing on singular minority\n",
    "cust_sum = deepcopy(retail)\n",
    "cust_sum.loc[cust_sum.cluster <= 1, \"class\"] = 0\n",
    "cust_sum.loc[cust_sum.cluster == 2, \"class\"] = 1\n",
    "cust_sum['class'] = cust_sum['class'].astype(\"int\")\n",
    "cust_sum['class'] = cust_sum['class'].astype(\"category\")\n",
    "cust_sum = cust_sum.drop(columns=['cluster'])\n",
    "print(cust_sum['class'].value_counts())\n",
    "cust_sum.name ='Retail Data -1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1935848\n",
      "1     269025\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cust Summary focusing on singular minority\n",
    "cust_sum2 = deepcopy(retail)\n",
    "cust_sum2.loc[cust_sum2.cluster ==0, \"class\"] = 0\n",
    "cust_sum2.loc[cust_sum2.cluster >= 1, \"class\"] = 1\n",
    "cust_sum2['class'] = cust_sum2['class'].astype(\"int\")\n",
    "cust_sum2['class'] = cust_sum2['class'].astype(\"category\")\n",
    "cust_sum2 = cust_sum2.drop(columns=['cluster'])\n",
    "print(cust_sum2['class'].value_counts())\n",
    "cust_sum2.name ='Retail Data -2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.877986\n",
       "1    0.122014\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_sum2['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OsUspqauau3P"
   },
   "outputs": [],
   "source": [
    "# train test/ scaling\n",
    "def data_prep (data, seed):\n",
    "  X= data.drop('class',axis=1).copy()\n",
    "  y = data['class'].copy()\n",
    "  y = y.astype('category')\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed, shuffle=y, stratify=y) # add ssed\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(X_train)\n",
    "  X_train_scaled = scaler.transform(X_train)\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns)\n",
    "  X_test_scaled = pd.DataFrame(X_test_scaled, columns = X_test.columns)\n",
    "\n",
    "  return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "bvOPEae0aw_a"
   },
   "outputs": [],
   "source": [
    "# random undersamping imbalance levels (high (5%), extreme (1%), absolute (6 samples)\n",
    "def random_under_minority (data, imbalance_level, seed):\n",
    "\n",
    "  random.seed(seed)\n",
    "  \n",
    "  X_train_scaled, X_test_scaled, y_train, y_test = data_prep(data, seed)\n",
    "  y_train = y_train.reset_index(drop=True)\n",
    "  X_train_scaled['class'] = y_train\n",
    "\n",
    "  class_counts = X_train_scaled['class'].value_counts().to_frame()\n",
    "  maj_count = class_counts.iloc[0,0]\n",
    "  min_count = class_counts.iloc[1,0]\n",
    "\n",
    "  majority = X_train_scaled[(X_train_scaled['class'] == 0)]\n",
    "  minority = X_train_scaled[(X_train_scaled['class']== 1)]\n",
    "\n",
    "  if imbalance_level == 'absolute':\n",
    "    downsample = 6\n",
    "  \n",
    "  else:\n",
    "    if imbalance_level == 'high':\n",
    "      imbalance = 0.05\n",
    "      \n",
    "    elif imbalance_level == 'extreme':\n",
    "      imbalance = 0.01\n",
    "\n",
    "    downsample = (maj_count * imbalance).round().astype('int')\n",
    "    \n",
    "    if imbalance_level == 'extreme' and downsample < 8:\n",
    "      downsample = 8\n",
    " \n",
    "  if downsample >= min_count:\n",
    "    minority_sample = minority\n",
    "      \n",
    "  else:\n",
    "    minority_sample = minority.sample(n= downsample)\n",
    "\n",
    "  final = pd.concat([majority, minority_sample])\n",
    "  final = shuffle(final)\n",
    "\n",
    "  X_train_scaled = final.drop('class',axis=1).copy()\n",
    "  y_train = final['class'].copy()\n",
    "  y_train = y_train.astype('category')\n",
    "\n",
    "  X_train_scaled = X_train_scaled\n",
    "  X_test_scaled = X_test_scaled\n",
    "  y_train = y_train.reset_index(drop=True)\n",
    "  y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "  return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "z8NQNtO3a1dX"
   },
   "outputs": [],
   "source": [
    "def SMOTE_sampling (data, imbalance, seed):\n",
    "  X_train_scaled, X_test_scaled, y_train, y_test = random_under_minority(data, imbalance, seed)\n",
    "\n",
    "  SMOTE_sample= SMOTE()\n",
    "  X_train_scaled, y_train = SMOTE_sample.fit_resample(X_train_scaled, y_train)\n",
    "  \n",
    "  return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "rTVFilIZa4PX"
   },
   "outputs": [],
   "source": [
    "def SWIM_sampling (data, imbalance, seed):\n",
    "  X_train_scaled, X_test_scaled, y_train, y_test = random_under_minority(data, imbalance, seed)\n",
    "  X_train_scaled = X_train_scaled.values\n",
    "  y_test = y_test.values\n",
    "\n",
    "  numSamples = np.sum(y_train==0)-np.sum(y_train==1)\n",
    "  sw = swim_maha.SwimMaha(sd= 2)\n",
    "  X_train_scaled, y_train = sw.mahaSampling(X_train_scaled, y_train, numSamples)\n",
    "  \n",
    "  return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "xtA3GN2Sa6ou"
   },
   "outputs": [],
   "source": [
    "def MAHAKIL_sampling (data, imbalance, seed):\n",
    "  X_train_scaled, X_test_scaled, y_train, y_test = random_under_minority(data, imbalance, seed)\n",
    "  \n",
    "  print('method start')\n",
    "  mk = mahakil.MAHAKIL(pfp=0.5)\n",
    "  print('method finished')\n",
    "    \n",
    "  X_train_scaled, y_train = mk.fit_sample(X_train_scaled, y_train)\n",
    "\n",
    "  maj_count = np.count_nonzero(y_train ==0)\n",
    "\n",
    "  df = pd.DataFrame(X_train_scaled)\n",
    "  df['class'] = y_train\n",
    "  df['class'] = df['class'].astype(int)\n",
    "\n",
    "  majority = df[(df['class'] == 0)]\n",
    "  minority = df[(df['class']== 1)]\n",
    "\n",
    "  minority = minority[0:maj_count]\n",
    "\n",
    "  final = pd.concat([majority, minority])\n",
    "  final = shuffle(final)\n",
    "\n",
    "  X_train_scaled = final.drop('class',axis=1).copy()\n",
    "  y_train = final['class'].copy()\n",
    "  y_train = y_train.astype('category')\n",
    "  \n",
    "  X_train_scaled = X_train_scaled\n",
    "  X_test_scaled = X_test_scaled\n",
    "  y_train = y_train.reset_index(drop=True)\n",
    "  y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "  return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "rRIAtvKez30l"
   },
   "outputs": [],
   "source": [
    "def classifier_results (data, imbalance, sampling_method, model, seed):\n",
    "  \n",
    "  if sampling_method == 'none':\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = random_under_minority(data, imbalance, seed)\n",
    "\n",
    "  if sampling_method == 'SMOTE':\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = SMOTE_sampling (data, imbalance, seed)\n",
    "  \n",
    "  if sampling_method == 'SWIM':\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = SWIM_sampling (data, imbalance, seed)\n",
    "  \n",
    "  if sampling_method == 'MAHAKIL':\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = MAHAKIL_sampling(data, imbalance, seed)\n",
    "\n",
    "  if model == 'naive_bayes':\n",
    "    model_func = GaussianNB()\n",
    "\n",
    "  elif model == 'K_neighbours':\n",
    "    model_func = KNeighborsClassifier(n_neighbors=3)\n",
    "  \n",
    "  elif model == 'Random_forest':\n",
    "    model_func = RandomForestClassifier()\n",
    "\n",
    "  model_base = model_func\n",
    "  model_base.fit(X_train_scaled, y_train)\n",
    "\n",
    "  y_pred = model_base.predict(X_test_scaled)\n",
    "\n",
    "  score_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "  AUC = roc_auc_score(y_test, y_pred)\n",
    "  GMS = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "  return [data.name, sampling_method, model, imbalance, score_f1, AUC, GMS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = random_under_minority(cust_sum2, 'high', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method start\n",
      "method finished\n",
      "sorted data\n",
      "calculated distance\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-5b72ce78ed78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimbalance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGMS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_results\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcust_sum2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'high'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MAHAKIL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'naive_bayes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-7f4fedb5e4cb>\u001b[0m in \u001b[0;36mclassifier_results\u001b[0;34m(data, imbalance, sampling_method, model, seed)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msampling_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MAHAKIL'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAHAKIL_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimbalance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'naive_bayes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-8295223cb7a0>\u001b[0m in \u001b[0;36mMAHAKIL_sampling\u001b[0;34m(data, imbalance, seed)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'method finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mmaj_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Peak_Diss/mahakil_update.py\u001b[0m in \u001b[0;36mfit_sample\u001b[0;34m(self, data, label)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Descending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;31m# Divide the set of positive examples into two\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0md_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "name, sampling_method, model, imbalance, score_f1, AUC, GMS = classifier_results (cust_sum2, 'high', 'MAHAKIL', 'naive_bayes', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retail Data -2', 'MAHAKIL', 'naive_bayes', 'high', 0.12724553172519965, 0.5031923751721629, 0.13503767579734224]\n"
     ]
    }
   ],
   "source": [
    "MAHAKIL = [name, sampling_method, model, imbalance, score_f1, AUC, GMS]\n",
    "print(NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retail Data -2', 'SWIM', 'naive_bayes', 'high', 0.12307679578857988, 0.5034337419332109, 0.1180404720840242]\n"
     ]
    }
   ],
   "source": [
    "SWIM = [name, sampling_method, model, imbalance, score_f1, AUC, GMS]\n",
    "print(SWIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retail Data -2', 'SMOTE', 'naive_bayes', 'high', 0.12337674798251844, 0.5033601136986904, 0.11938551997025057]\n"
     ]
    }
   ],
   "source": [
    "SMOTE = [name, sampling_method, model, imbalance, score_f1, AUC, GMS]\n",
    "print(SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retail Data -2', 'MAHAKIL', 'naive_bayes', 'high', 0.12724553172519965, 0.5031923751721629, 0.13503767579734224]\n"
     ]
    }
   ],
   "source": [
    "NONE = [name, sampling_method, model, imbalance, score_f1, AUC, GMS]\n",
    "print(MAHAKIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "n6zh7vlQfgjd",
    "outputId": "02179e79-5d5f-45a8-c5e5-2c23dee92830"
   },
   "outputs": [],
   "source": [
    "all_results = []\n",
    "datasets = [cust_sum, cust_sum2]\n",
    "imbalance = ['high', 'extreme', 'absolute']\n",
    "sampling_method = ['none', 'SMOTE','SWIM', 'MAHAKIL']\n",
    "models = ['naive_bayes', 'K_neighbours', 'Random_forest']\n",
    "for data in datasets:\n",
    "  for i in imbalance: \n",
    "    for s in sampling_method:\n",
    "      for m in models:\n",
    "        for seed in range(30):\n",
    "            print(seed)\n",
    "            try:\n",
    "                results = classifier_results (data, i, s, m, seed) \n",
    "                all_results.append(results) \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "df = pd.DataFrame(all_results, columns=['Data', 'Sampling_method','Model', 'Imbalance_level', 'F1', 'AUC', 'G_mean'])   \n",
    "df.to_csv('retail_results.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "Iy8R7KGDghK3",
    "outputId": "ecf82cb2-ef31-4ad7-84d6-0dbbd4c99a5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sampling_method</th>\n",
       "      <th>Data</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>G_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAHAKIL</td>\n",
       "      <td>D10 - Wine_white_LvH</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAHAKIL</td>\n",
       "      <td>D3 - Vowel_0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAHAKIL</td>\n",
       "      <td>D4 - Vowel_2</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>D10 - Wine_white_LvH</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>D3 - Vowel_0</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>D4 - Vowel_2</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SWIM</td>\n",
       "      <td>D10 - Wine_white_LvH</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SWIM</td>\n",
       "      <td>D3 - Vowel_0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SWIM</td>\n",
       "      <td>D4 - Vowel_2</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>none</td>\n",
       "      <td>D10 - Wine_white_LvH</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>none</td>\n",
       "      <td>D3 - Vowel_0</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>none</td>\n",
       "      <td>D4 - Vowel_2</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sampling_method                  Data     F1    AUC  G_mean\n",
       "0          MAHAKIL  D10 - Wine_white_LvH  0.792  0.780   0.757\n",
       "1          MAHAKIL          D3 - Vowel_0  0.725  0.754   0.696\n",
       "2          MAHAKIL          D4 - Vowel_2  0.900  0.909   0.903\n",
       "3            SMOTE  D10 - Wine_white_LvH  0.790  0.790   0.772\n",
       "4            SMOTE          D3 - Vowel_0  0.732  0.767   0.728\n",
       "5            SMOTE          D4 - Vowel_2  0.887  0.912   0.908\n",
       "6             SWIM  D10 - Wine_white_LvH  0.783  0.804   0.794\n",
       "7             SWIM          D3 - Vowel_0  0.735  0.788   0.754\n",
       "8             SWIM          D4 - Vowel_2  0.804  0.925   0.921\n",
       "9             none  D10 - Wine_white_LvH  0.727  0.680   0.598\n",
       "10            none          D3 - Vowel_0  0.701  0.695   0.541\n",
       "11            none          D4 - Vowel_2  0.815  0.769   0.717"
      ]
     },
     "execution_count": 205,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = df.copy(deep=True)\n",
    "high = group[(group['Imbalance_level']== 'high')]\n",
    "high = high.groupby(\n",
    "   ['Sampling_method', 'Data']\n",
    ").agg(\n",
    "    {  \n",
    "         'F1': \"mean\",  \n",
    "         'AUC': 'mean',\n",
    "         'G_mean': \"mean\",  \n",
    "    }\n",
    ").round(decimals=3)\n",
    "high = high.reset_index()\n",
    "high"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EmmaDiss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
